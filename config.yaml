training:
  batch_size: 8       # Example value
  epochs: 2        # Example value
  start_epoch: 1       # Example value
  lambda_L1: 10.0      # Example value for L1 regularization weight
  lr: 0.0003           # Learning rate for Adam optimizer
  checkpoint_interval: 20  # How often to save checkpoints
  max_grad_norm: 1.0  # Add this line
  early_stopping_patience: 7    # Add this line
  early_stopping_min_delta: 0.0 # Add this line
  scheduler:
    factor: 0.5      # Learning rate reduction factor
    patience: 5      # Number of epochs to wait before reducing LR
    min_lr: 0.00001  # Minimum learning rate

# 1: CMP Facade Database; 
dataset: 1

data:
  datasets_dir: "data/dataset_"
  train_images_dir: "processed/train/images"
  train_labels_dir: "processed/train/labels"
  test_images_dir: "processed/test/images"
  test_labels_dir: "processed/test/labels"
  val_images_dir: "processed/val/images"
  val_labels_dir: "processed/val/labels"

logging:
  log_interval: 1                 # Log frequency (in steps)
  checkpoint_interval: 1           # Frequency of saving model checkpoints
  validation_interval: 1
  val_samples_interval: 1
  checkpoint_dir: "checkpoints/prototyp2"

device:
  use_gpu: true                    # Enable GPU usage if available
