training:
  batch_size: 16       # Example value
  epochs: 200        # Example value
  start_epoch: 1       # Example value
  lambda_L1: 10.0      # Example value for L1 regularization weight
  lr: 0.0002           # Learning rate for Adam optimizer
  checkpoint_interval: 20  # How often to save checkpoints
  max_grad_norm: 1  # Add this line
  early_stopping_patience: 7    # Add this line
  early_stopping_min_delta: 0.0 # Add this line
  scheduler:
    factor: 0.5      # Learning rate reduction factor
    patience: 5      # Number of epochs to wait before reducing LR
    min_lr: 0.00001  # Minimum learning rate


data:
  train_images_dir: "data/processed_test/train/images"
  train_labels_dir: "data/processed_test/train/labels"
  test_images_dir: "data/processed/test/images"
  test_labels_dir: "data/processed/test/labels"
  val_images_dir: "data/processed/val/images"
  val_labels_dir: "data/processed/val/labels"

logging:
  log_interval: 10                 # Log frequency (in steps)
  checkpoint_interval: 201           # Frequency of saving model checkpoints
  validation_interval: 1 
  checkpoint_dir: "checkpoints/prototyp_28"

device:
  use_gpu: true                    # Enable GPU usage if available
